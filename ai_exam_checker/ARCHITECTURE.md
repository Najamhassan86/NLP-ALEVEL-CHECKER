# System Architecture - AI Exam Checker

## High-Level Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         USER INTERFACE                            â”‚
â”‚                      Streamlit Web App                            â”‚
â”‚                     http://localhost:8501                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚ HTTP REST API
                             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                       FASTAPI BACKEND                             â”‚
â”‚                     http://localhost:8000                         â”‚
â”‚                                                                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚              EVALUATION ORCHESTRATOR                     â”‚   â”‚
â”‚  â”‚            (app/main.py - /evaluate)                     â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜   â”‚
â”‚       â”‚                                              â”‚          â”‚
â”‚       â–¼                                              â–¼          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚   RAG PIPELINE     â”‚                    â”‚  PERSISTENCE     â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## RAG Pipeline Detail

### Stage 1: Embedding & Indexing (Offline)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Marking Schemes    â”‚
â”‚  (.md files)        â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Document Loader    â”‚
â”‚  (ingest.py)        â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚     Chunker         â”‚  Strategy: Criterion-based
â”‚  (app/rag.py)       â”‚  - Split by bullet points
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  - Split by numbered items
       â”‚                  - Preserve metadata
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Embedding Generator â”‚  Model: all-MiniLM-L6-v2
â”‚ (app/embeddings.py) â”‚  Dimension: 384
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    ChromaDB         â”‚  Persistent vector store
â”‚  ./chroma_db/       â”‚  Cosine similarity search
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Stage 2: Retrieval (Online - per evaluation)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Student Answer     â”‚
â”‚  (User Input)       â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Embedding Generator â”‚  Convert to 384-dim vector
â”‚ (same model)        â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Vector Similarity   â”‚  Query ChromaDB
â”‚    Search           â”‚  Top-K retrieval (K=5)
â”‚                     â”‚  Filter: subject + question_id
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  Threshold: 0.3 similarity
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Retrieved Context   â”‚  5 most relevant criteria
â”‚ (RetrievalResult[]) â”‚  + similarity scores
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Stage 3: LLM Evaluation

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Student Answer     â”‚
â”‚        +            â”‚
â”‚ Retrieved Context   â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Prompt Builder     â”‚  Constrained prompting:
â”‚ (app/evaluation.py) â”‚  - Only use retrieved context
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  - Output structured JSON
       â”‚                  - Explain all scores
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Ollama LLM        â”‚  Model: llama3.1:8b
â”‚ localhost:11434     â”‚  Temperature: 0.1 (deterministic)
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  JSON Response      â”‚  {
â”‚                     â”‚    criteria_evaluations: [
â”‚                     â”‚      {criterion, marks, justification}
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    ]
                           }
```

### Stage 4: Scoring & Feedback

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Criterion Scores    â”‚  Parsed from LLM JSON
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Scoring Engine     â”‚  - Aggregate total marks
â”‚  (app/scoring.py)   â”‚  - Validate consistency
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  - Identify strengths/weaknesses
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Feedback Generator  â”‚  - Generate summary
â”‚ (app/feedback.py)   â”‚  - Improvement suggestions
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  - Grade assignment
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Evaluation Response â”‚  Complete results
â”‚                     â”‚  (EvaluationResponse)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Data Models

### Core Data Flow

```
EvaluationRequest
  â†“
RetrievalResult[] â”€â”€â”
                     â”œâ”€â†’ LLM
Student Answer â”€â”€â”€â”€â”€â”€â”˜
  â†“
CriterionScore[]
  â†“
EvaluationResponse
  â†“
SQLite Database
```

### Key Pydantic Models

```python
EvaluationRequest {
  subject: str
  question_id: str
  student_answer: str
}

RetrievalResult {
  content: str
  metadata: dict
  similarity_score: float
}

CriterionScore {
  criterion: str
  max_marks: int
  awarded_marks: int
  justification: str
  missing_points: list[str]
}

EvaluationResponse {
  subject: str
  question_id: str
  student_answer: str
  retrieved_context: RetrievalResult[]
  criteria_scores: CriterionScore[]
  total_marks_awarded: int
  total_marks_possible: int
  feedback: str
  strengths: list[str]
  weaknesses: list[str]
  improvement_suggestions: list[str]
  timestamp: datetime
}
```

---

## Component Interactions

### Singleton Pattern (Performance Optimization)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         Global Instances (Cached)         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  EmbeddingGenerator  (loaded once)       â”‚
â”‚  VectorRetriever     (shared connection) â”‚
â”‚  AnswerEvaluator     (Ollama client)     â”‚
â”‚  ScoringEngine       (stateless)         â”‚
â”‚  FeedbackGenerator   (stateless)         â”‚
â”‚  Database            (connection pool)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### API Endpoint Flow

```
POST /evaluate
  â”‚
  â”œâ”€â†’ get_retriever()
  â”‚    â””â”€â†’ Semantic search
  â”‚
  â”œâ”€â†’ get_evaluator()
  â”‚    â””â”€â†’ LLM call (Ollama)
  â”‚
  â”œâ”€â†’ get_scoring_engine()
  â”‚    â””â”€â†’ Score aggregation
  â”‚
  â”œâ”€â†’ get_feedback_generator()
  â”‚    â””â”€â†’ Feedback synthesis
  â”‚
  â””â”€â†’ get_database()
       â””â”€â†’ Save results
```

---

## Technology Stack Rationale

### Why ChromaDB?
- âœ… **Persistent local storage** (no cloud required)
- âœ… **Built-in embedding support**
- âœ… **Easy metadata filtering**
- âœ… **Windows compatible**
- âœ… **No configuration needed**

### Why Ollama?
- âœ… **Fully local inference** (privacy)
- âœ… **Easy Windows installation**
- âœ… **Multiple model support**
- âœ… **HTTP API** (language agnostic)
- âœ… **Automatic GPU acceleration**

### Why sentence-transformers?
- âœ… **Lightweight models** (~100MB)
- âœ… **Fast inference** (CPU friendly)
- âœ… **High quality semantic embeddings**
- âœ… **Pre-trained on semantic similarity**

### Why FastAPI?
- âœ… **Async support** (future scalability)
- âœ… **Auto-generated API docs**
- âœ… **Type validation** (Pydantic)
- âœ… **Modern Python framework**

### Why Streamlit?
- âœ… **Rapid development** (minimal code)
- âœ… **Python-native** (no JS required)
- âœ… **Interactive widgets**
- âœ… **Perfect for demos/research**

---

## Performance Characteristics

### Latency Breakdown (Typical)

```
Total: ~10-20 seconds
â”œâ”€ Embedding generation:    0.1-0.3s
â”œâ”€ Vector retrieval:        0.05-0.15s
â”œâ”€ LLM inference:           5-15s     â—„â”€â”€ Bottleneck
â”œâ”€ Score calculation:       <0.01s
â”œâ”€ Feedback generation:     <0.01s
â””â”€ Database save:           <0.05s
```

### Memory Usage

```
Base (Python + packages):    ~500MB
Embedding model:             ~100MB
ChromaDB index:              ~10MB (per 100 docs)
Ollama (llama3.1:8b):        ~4-6GB
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Total:                       ~5-7GB
```

### Storage

```
./chroma_db/           ~5-10MB  (vectors)
./exam_results.db      ~1-5MB   (per 1000 evaluations)
Models:                ~4.7GB   (Ollama)
                       ~100MB   (embeddings)
```

---

## Security & Privacy

### Data Flow
- âœ… All data stays local (Windows machine)
- âœ… No internet required (after setup)
- âœ… No telemetry or tracking
- âœ… No API keys needed

### Access Control
- ğŸ”¸ No authentication (local use only)
- ğŸ”¸ API accessible to localhost only
- ğŸ”¸ Not production-ready for public deployment

---

## Extensibility Points

### Add New Embedding Model
```python
# app/settings.py
EMBEDDING_MODEL = "sentence-transformers/all-mpnet-base-v2"
```

### Add New LLM
```python
# app/settings.py
OLLAMA_MODEL = "mistral" or "llama2" or "codellama"
```

### Custom Chunking Strategy
```python
# app/rag.py - Chunker class
def chunk_by_semantic_similarity(self, text, metadata):
    # Implement semantic chunking
    pass
```

### Add Re-ranking
```python
# After retrieval, before LLM
from sentence_transformers import CrossEncoder
reranker = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')
scores = reranker.predict([(query, doc) for doc in retrieved])
```

---

## Error Handling Strategy

```
User Input
  â†“
Validation (Pydantic) â”€â”€â†’ 422 Error
  â†“
Retrieval â”€â”€â†’ No results? â”€â”€â†’ 404 Error
  â†“
LLM Evaluation â”€â”€â†’ Timeout/Error? â”€â”€â†’ 500 Error
  â†“              â””â”€â†’ Log & return low confidence
Score Validation â”€â”€â†’ Warnings logged
  â†“
Success Response (200)
```

---

## Future Enhancements (Research Directions)

1. **Hybrid Search**: Combine dense (vector) + sparse (BM25) retrieval
2. **Re-ranking**: Two-stage retrieval with cross-encoders
3. **Query Expansion**: Rephrase student answers for better retrieval
4. **Chain-of-Thought**: Multi-step reasoning for complex answers
5. **Confidence Calibration**: Better uncertainty estimation
6. **Multi-modal**: Support image-based answers
7. **Active Learning**: Improve from feedback

---

**This architecture prioritizes transparency, local execution, and research clarity over production scalability.**
